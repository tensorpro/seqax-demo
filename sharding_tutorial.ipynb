{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1209a7",
   "metadata": {},
   "source": [
    "# Sharding & Tiling Matmuls — A Hands-On Tutorial\n",
    "\n",
    "Shows that sharded matmuls produce the same results as numpy.\n",
    "We simulate 8 devices on CPU so we can print per-device shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9248c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices available: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "\n",
    "import inspect\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.experimental.mesh_utils import create_device_mesh\n",
    "from jax.sharding import Mesh\n",
    "\n",
    "from shardlib import shardops\n",
    "from shardlib.shardtypes import f32, make_partition_specs, Scope\n",
    "\n",
    "def typed_shard_map(f, **kwargs):\n",
    "    \"\"\"Like shardtypes.typed_shard_map but without typechecked (works in notebooks).\"\"\"\n",
    "    sig = inspect.signature(f)\n",
    "    def wrapped(*args):\n",
    "        mesh = jax._src.mesh.thread_resources.env.physical_mesh\n",
    "        in_specs = tuple(make_partition_specs(p.annotation) for p in sig.parameters.values())\n",
    "        out_specs = make_partition_specs(sig.return_annotation)\n",
    "        return jax.experimental.shard_map.shard_map(\n",
    "            f, in_specs=in_specs, out_specs=out_specs, mesh=mesh, **kwargs\n",
    "        )(*args)\n",
    "    return wrapped\n",
    "\n",
    "print(f\"Devices available: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4d445",
   "metadata": {},
   "source": [
    "## Part 1: Sharding Non-Contracting Dimensions\n",
    "\n",
    "`Y = X @ W` contracts over K. Sharding M (rows of X) or N (cols of W) needs\n",
    "no communication — each device just computes its own slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2736b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[[5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "w = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "\n",
      "result = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[[1. 2. 1. 2.]\n",
      " [3. 4. 3. 4.]]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[[5. 6. 5. 6.]\n",
      " [7. 8. 7. 8.]]\n",
      "\n",
      "\n",
      "sharded result:\n",
      " [[1. 2. 1. 2.]\n",
      " [3. 4. 3. 4.]\n",
      " [5. 6. 5. 6.]\n",
      " [7. 8. 7. 8.]]\n",
      "numpy result:\n",
      " [[1. 2. 1. 2.]\n",
      " [3. 4. 3. 4.]\n",
      " [5. 6. 5. 6.]\n",
      " [7. 8. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "# Row-sharded: M/d — each device gets a subset of rows of X\n",
    "d = 2\n",
    "\n",
    "with Mesh(np.array(jax.devices()[:d]).reshape(d), (\"d\",)):\n",
    "    X = jnp.array([[1, 2],\n",
    "                    [3, 4],\n",
    "                    [5, 6],\n",
    "                    [7, 8]], dtype=jnp.float32)  # (4, 2)\n",
    "    W = jnp.array([[1, 0, 1, 0],\n",
    "                    [0, 1, 0, 1]], dtype=jnp.float32)  # (2, 4)\n",
    "\n",
    "    with Scope():\n",
    "        def fn(x: f32[b\"M/d K\"], w: f32[b\"K N\"]) -> f32[b\"M/d N\"]:\n",
    "            print(\"x =\", x)\n",
    "            print(\"w =\", w)\n",
    "            result = shardops.einsum_unreduced(\"M/d K, K N -> M/d N\", x, w)\n",
    "            print(\"result =\", result)\n",
    "            return result\n",
    "        sharded = typed_shard_map(fn, check_rep=False)(X, W)\n",
    "\n",
    "    print(\"\\nsharded result:\\n\", sharded)\n",
    "    print(\"numpy result:\\n\", np.array(X) @ np.array(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297fdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = On TFRT_CPU_0 at mesh coordinates (t,) = (0,):\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (t,) = (1,):\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "w = On TFRT_CPU_0 at mesh coordinates (t,) = (0,):\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (t,) = (1,):\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "result = On TFRT_CPU_0 at mesh coordinates (t,) = (0,):\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (t,) = (1,):\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "\n",
      "\n",
      "sharded result:\n",
      " [[1. 2. 1. 2.]\n",
      " [3. 4. 3. 4.]\n",
      " [5. 6. 5. 6.]\n",
      " [7. 8. 7. 8.]]\n",
      "numpy result:\n",
      " [[1. 2. 1. 2.]\n",
      " [3. 4. 3. 4.]\n",
      " [5. 6. 5. 6.]\n",
      " [7. 8. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "# Column-sharded: N/t — each device gets a subset of columns of W\n",
    "t = 2\n",
    "\n",
    "with Mesh(np.array(jax.devices()[:t]).reshape(t), (\"t\",)):\n",
    "    X = jnp.array([[1, 2],\n",
    "                    [3, 4],\n",
    "                    [5, 6],\n",
    "                    [7, 8]], dtype=jnp.float32)  # (4, 2)\n",
    "    W = jnp.array([[1, 0, 1, 0],\n",
    "                    [0, 1, 0, 1]], dtype=jnp.float32)  # (2, 4)\n",
    "\n",
    "    with Scope():\n",
    "        def fn(x: f32[b\"M K\"], w: f32[b\"K N/t\"]) -> f32[b\"M N/t\"]:\n",
    "            print(\"x =\", x)\n",
    "            print(\"w =\", w)\n",
    "            result = shardops.einsum_unreduced(\"M K, K N/t -> M N/t\", x, w)\n",
    "            print(\"result =\", result)\n",
    "            return result\n",
    "        sharded = typed_shard_map(fn, check_rep=False)(X, W)\n",
    "\n",
    "    print(\"\\nsharded result:\\n\", sharded)\n",
    "    print(\"numpy result:\\n\", np.array(X) @ np.array(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c4dc7",
   "metadata": {},
   "source": [
    "## Part 2: Sharding the Contracting Dimension\n",
    "\n",
    "When K is sharded, each device computes a partial matmul. We need\n",
    "`psum` or `psum_scatter` to sum the partial results across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7057d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[1. 2.]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[3. 4.]\n",
      "\n",
      "b = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[2. 1.]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[2. 1.]\n",
      "\n",
      "partial (before psum) = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[4.]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[10.]\n",
      "\n",
      "result (after psum) = On TFRT_CPU_0 at mesh coordinates (d,) = (0,):\n",
      "[14.]\n",
      "\n",
      "On TFRT_CPU_1 at mesh coordinates (d,) = (1,):\n",
      "[14.]\n",
      "\n",
      "\n",
      "sharded result: 14.0\n",
      "numpy result: 14.0\n"
     ]
    }
   ],
   "source": [
    "# Sharded dot product: K/d — each device gets a slice of the vectors\n",
    "d = 2\n",
    "\n",
    "with Mesh(np.array(jax.devices()[:d]).reshape(d), (\"d\",)):\n",
    "    a = jnp.array([1, 2, 3, 4], dtype=jnp.float32)  # (4,)\n",
    "    b = jnp.array([2, 1, 2, 1], dtype=jnp.float32)  # (4,)\n",
    "\n",
    "    with Scope():\n",
    "        def fn(a: f32[b\"K/d\"], b: f32[b\"K/d\"]) -> f32[b\"K/d\"]:\n",
    "            print(\"a =\", a)\n",
    "            print(\"b =\", b)\n",
    "            partial = jnp.sum(a * b, keepdims=True)\n",
    "            print(\"partial (before psum) =\", partial)\n",
    "            result = jax.lax.psum(partial, \"d\")\n",
    "            print(\"result (after psum) =\", result)\n",
    "            return jnp.broadcast_to(result, a.shape)\n",
    "        sharded = typed_shard_map(fn, check_rep=False)(a, b)\n",
    "\n",
    "    print(\"\\nsharded result:\", sharded[0])\n",
    "    print(\"numpy result:\", np.dot(np.array(a), np.array(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-sharded matmul: same idea but for matrices\n",
    "d = 2\n",
    "\n",
    "with Mesh(np.array(jax.devices()[:d]).reshape(d), (\"d\",)):\n",
    "    X2 = jnp.array([[1, 2, 3, 4],\n",
    "                     [5, 6, 7, 8]], dtype=jnp.float32)  # (2, 4)\n",
    "    W2 = jnp.array([[1, 0],\n",
    "                     [0, 1],\n",
    "                     [1, 0],\n",
    "                     [0, 1]], dtype=jnp.float32)  # (4, 2)\n",
    "\n",
    "    with Scope():\n",
    "        def fn(x: f32[b\"M K/d\"], w: f32[b\"K/d N\"]) -> f32[b\"M N\"]:\n",
    "            print(\"x =\", x)\n",
    "            print(\"w =\", w)\n",
    "            partial = shardops.einsum_unreduced(\"M K/d, K/d N -> M N\", x, w)\n",
    "            print(\"partial (before psum) =\", partial)\n",
    "            result = jax.lax.psum(partial, \"d\")\n",
    "            print(\"result (after psum) =\", result)\n",
    "            return result\n",
    "        sharded = typed_shard_map(fn, check_rep=False)(X2, W2)\n",
    "\n",
    "    print(\"\\nsharded result:\\n\", sharded)\n",
    "    print(\"numpy result:\\n\", np.array(X2) @ np.array(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa30213",
   "metadata": {},
   "source": [
    "## Part 3: Megatron-Style MLP\n",
    "\n",
    "`y = relu(x @ W1) @ W2` — W1 column-sharded (`F/t`), W2 row-sharded (`F/t`).\n",
    "Intermediate `(B, F/t)` feeds directly into W2. Only one `psum` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2\n",
    "\n",
    "with Mesh(np.array(jax.devices()[:t]).reshape(t), (\"t\",)):\n",
    "    x_mlp = jnp.array([[1, 1],\n",
    "                        [1, 1],\n",
    "                        [1, 1],\n",
    "                        [1, 1]], dtype=jnp.float32)  # (B=4, M=2)\n",
    "    W1 = jnp.array([[0.5, 0.5, 0.5, 0.5],\n",
    "                     [0.5, 0.5, 0.5, 0.5]], dtype=jnp.float32)  # (M=2, F=4)\n",
    "    W2 = jnp.array([[0.5, 0.5],\n",
    "                     [0.5, 0.5],\n",
    "                     [0.5, 0.5],\n",
    "                     [0.5, 0.5]], dtype=jnp.float32)  # (F=4, M=2)\n",
    "\n",
    "    with Scope():\n",
    "        def fn(x: f32[b\"B Mdl\"], w1: f32[b\"Mdl F/t\"], w2: f32[b\"F/t Mdl\"]) -> f32[b\"B Mdl\"]:\n",
    "            print(\"x =\", x)\n",
    "            print(\"w1 =\", w1)\n",
    "            print(\"w2 =\", w2)\n",
    "            h = jax.nn.relu(jnp.einsum(\"bm,mf->bf\", x, w1))\n",
    "            print(\"h (after layer 1, no comm needed) =\", h)\n",
    "            y_partial = jnp.einsum(\"bf,fm->bm\", h, w2)\n",
    "            print(\"y_partial (before psum) =\", y_partial)\n",
    "            result = jax.lax.psum(y_partial, \"t\")\n",
    "            print(\"result (after psum) =\", result)\n",
    "            return result\n",
    "        sharded = typed_shard_map(fn, check_rep=False)(x_mlp, W1, W2)\n",
    "\n",
    "    print(\"\\nsharded result:\\n\", sharded)\n",
    "    print(\"numpy result:\\n\", np.maximum(0, np.array(x_mlp) @ np.array(W1)) @ np.array(W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa645b",
   "metadata": {},
   "source": [
    "## Part 4: Tiled Matmul (no K-split)\n",
    "\n",
    "Each output tile `Y[i,j] = X[i_rows, :] @ W[:, j_cols]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = jnp.array([[ 1,  2,  3],\n",
    "                  [ 4,  5,  6],\n",
    "                  [ 7,  8,  9],\n",
    "                  [10, 11, 12]], dtype=jnp.float32)  # (4, 3)\n",
    "W_t = jnp.array([[1, 0, 1, 0],\n",
    "                  [0, 1, 0, 1],\n",
    "                  [1, 1, 0, 0]], dtype=jnp.float32)  # (3, 4)\n",
    "\n",
    "def tiled_matmul(X, W, m, n):\n",
    "    M, K = X.shape\n",
    "    _, N = W.shape\n",
    "    Y = jnp.zeros((M, N))\n",
    "    for i in range(M // m):\n",
    "        for j in range(N // n):\n",
    "            Y = Y.at[i*m:(i+1)*m, j*n:(j+1)*n].set(\n",
    "                X[i*m:(i+1)*m, :] @ W[:, j*n:(j+1)*n])\n",
    "    return Y\n",
    "\n",
    "print(\"tiled:\\n\", tiled_matmul(X_t, W_t, 2, 2))\n",
    "print(\"numpy:\\n\", np.array(X_t) @ np.array(W_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d8e9e",
   "metadata": {},
   "source": [
    "## Part 5: Tiled Matmul with K-splitting\n",
    "\n",
    "Split K into chunks, accumulate partial products.\n",
    "This maps to `@pallas_call` + `@sequentially` in `kernelops.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ed666",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2 = jax.random.normal(jax.random.PRNGKey(0), (8, 6))\n",
    "W_t2 = jax.random.normal(jax.random.PRNGKey(1), (6, 4))\n",
    "\n",
    "def tiled_matmul_k(X, W, m, n, k):\n",
    "    M, K = X.shape\n",
    "    _, N = W.shape\n",
    "    Y = jnp.zeros((M, N))\n",
    "    for i in range(M // m):\n",
    "        for j in range(N // n):\n",
    "            acc = jnp.zeros((m, n))\n",
    "            for kk in range(K // k):\n",
    "                acc = acc + X[i*m:(i+1)*m, kk*k:(kk+1)*k] @ W[kk*k:(kk+1)*k, j*n:(j+1)*n]\n",
    "            Y = Y.at[i*m:(i+1)*m, j*n:(j+1)*n].set(acc)\n",
    "    return Y\n",
    "\n",
    "result = tiled_matmul_k(X_t2, W_t2, 4, 2, 3)\n",
    "print(\"tiled:\\n\", result)\n",
    "print(\"numpy:\\n\", np.array(X_t2) @ np.array(W_t2))\n",
    "print(f\"match? {jnp.allclose(result, np.array(X_t2) @ np.array(W_t2), atol=1e-4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
